{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8083792,"sourceType":"datasetVersion","datasetId":4761298},{"sourceId":8252468,"sourceType":"datasetVersion","datasetId":4896822},{"sourceId":8252629,"sourceType":"datasetVersion","datasetId":4896945}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-28T12:40:57.775753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nHOME = os.getcwd()\nprint(HOME)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n!nvcc --version\nTORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\nCUDA_VERSION = torch.__version__.split(\"+\")[-1]\nprint(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics\n\nfrom IPython import display\ndisplay.clear_output()\n\nimport ultralytics\nultralytics.checks()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n\nfrom IPython import display\ndisplay.clear_output()\n\nimport detectron2\nprint(\"detectron2:\", detectron2.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install supervision==0.2.0\n\nfrom IPython import display\ndisplay.clear_output()\n\nimport supervision as sv\nprint(\"supervision\", sv.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('yolov8s.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport supervision as sv\nnp.bool = np.bool_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5x6')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd {HOME}\n!git clone https://github.com/ultralytics/yolov5\n\n%cd {HOME}/yolov5\n!pip install -r requirements.txt\n\nfrom IPython import display\ndisplay.clear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd {HOME}\n!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vVrEVMxucHgqGd7vAa501ASojbeGPhIr' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vVrEVMxucHgqGd7vAa501ASojbeGPhIr\" -O market-square.mp4 && rm -rf /tmp/cookies.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MARKET_SQUARE_VIDEO_PATH = f\"{HOME}/market-square.mp4\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport supervision as sv\nnp.bool = np.bool_\n\n# extract video frame\ngenerator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\niterator = iter(generator)\nframe = next(iterator)\n\n# detect\nresults = model(frame, size=1280)\ndetections = sv.Detections.from_yolov5(results)\n\n# annotate\nbox_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\nframe = box_annotator.annotate(scene=frame, detections=detections)\n\n%matplotlib inline\nsv.show_frame_in_notebook(frame, (16, 16))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = sv.ColorPalette.default()\npolygons = [\n    np.array([\n        [540,  985 ],\n        [1620, 985 ],\n        [2160, 1920],\n        [1620, 2855],\n        [540,  2855],\n        [0,    1920]\n    ], np.int32),\n    np.array([\n        [0,    1920],\n        [540,  985 ],\n        [0,    0   ]\n    ], np.int32),\n    np.array([\n        [1620, 985 ],\n        [2160, 1920],\n        [2160,    0]\n    ], np.int32),\n    np.array([\n        [540,  985 ],\n        [0,    0   ],\n        [2160, 0   ],\n        [1620, 985 ]\n    ], np.int32),\n    np.array([\n        [0,    1920],\n        [0,    3840],\n        [540,  2855]\n    ], np.int32),\n    np.array([\n        [2160, 1920],\n        [1620, 2855],\n        [2160, 3840]\n    ], np.int32),\n    np.array([\n        [1620, 2855],\n        [540,  2855],\n        [0,    3840],\n        [2160, 3840]\n    ], np.int32)\n]\nvideo_info = sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)\n\nzones = [\n    sv.PolygonZone(\n        polygon=polygon, \n        frame_resolution_wh=video_info.resolution_wh\n    )\n    for polygon\n    in polygons\n]\nzone_annotators = [\n    sv.PolygonZoneAnnotator(\n        zone=zone, \n        color=colors.by_idx(index), \n        thickness=6,\n        text_thickness=8,\n        text_scale=4\n    )\n    for index, zone\n    in enumerate(zones)\n]\nbox_annotators = [\n    sv.BoxAnnotator(\n        color=colors.by_idx(index), \n        thickness=4, \n        text_thickness=4, \n        text_scale=2\n        )\n    for index\n    in range(len(polygons))\n]\n\n# extract video frame\ngenerator = sv.get_video_frames_generator(MARKET_SQUARE_VIDEO_PATH)\niterator = iter(generator)\nframe = next(iterator)\n\n# detect\nresults = model(frame, size=1280)\ndetections = sv.Detections.from_yolov5(results)\ndetections = detections[(detections.class_id == 0) & (detections.confidence > 0.5)]\n\nfor zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n    mask = zone.trigger(detections=detections)\n    detections_filtered = detections[mask]\n    frame = box_annotator.annotate(scene=frame, detections=detections_filtered, skip_label=True)\n    frame = zone_annotator.annotate(scene=frame)\n\n%matplotlib inline  \nsv.show_frame_in_notebook(frame, (16, 16))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = r\"/kaggle/input/video11/2766950-hd_1920_1080_30fps.mp4\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport supervision as sv\nnp.bool = np.bool_\n\n# extract video frame\ngenerator = sv.get_video_frames_generator(data)\niterator = iter(generator)\nframe = next(iterator)\n\n# detect\nresults = model(frame, size=1280)\ndetections = sv.Detections.from_yolov5(results)\n\n# Print all detected classes\nprint(\"Detected Classes:\")\nfor cls in detections.class_id:\n    print(cls)\n\n# annotate\nbox_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\nframe = box_annotator.annotate(scene=frame, detections=detections)\n\n%matplotlib inline\nsv.show_frame_in_notebook(frame, (16, 16))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2 = r\"/kaggle/input/presen00/41002-427854627.mp4\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport supervision as sv\nnp.bool = np.bool_\n\n# extract video frame\ngenerator = sv.get_video_frames_generator(data2)\niterator = iter(generator)\nframe = next(iterator)\n\n# detect\nresults = model(frame, size=1280)\ndetections = sv.Detections.from_yolov5(results)\n\n# Print all detected classes\nprint(\"Detected Classes:\")\nfor cls in detections.class_id:\n    print(cls)\n\n# annotate\nbox_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\nframe = box_annotator.annotate(scene=frame, detections=detections)\n\n%matplotlib inline\nsv.show_frame_in_notebook(frame, (16, 16))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = r\"/kaggle/input/presen00/STABLE-2.mp4\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport supervision as sv\nnp.bool = np.bool_\n\n# extract video frame\ngenerator = sv.get_video_frames_generator(data3)\niterator = iter(generator)\nframe = next(iterator)\n\n# detect\nresults = model(frame, size=1280)\ndetections = sv.Detections.from_yolov5(results)\n\n# Print all detected classes\nprint(\"Detected Classes:\")\nfor cls in detections.class_id:\n    print(cls)\n\n# annotate\nbox_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\nframe = box_annotator.annotate(scene=frame, detections=detections)\n\n%matplotlib inline\nsv.show_frame_in_notebook(frame, (16, 16))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://cdn.pixabay.com/video/2020/06/03/40999-427854604_medium.mp4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data3 = r\"/kaggle/working/40999-427854604_medium.mp4\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport supervision as sv\nnp.bool = np.bool_\n\ncolors = sv.ColorPalette.default()\n# import numpy as np\n\n# import numpy as np\n\n# coordinates = [\n#     np.array([\n#         [38, 78],[22, 546],[666, 510],[618, 58],[30, 82],\n#         [34, 1042],[774, 1018],[838, 510],[654, 514],[1106, 478],\n#         [966, 54],[618, 58],[1749, 34],[1772, 650],[1105, 466],\n#         [1212, 898],[777, 1022],[1828, 1026],[1776, 650]\n#     ])\n# ]\n\n# # polygons = []\n\n# # for coord in coordinates:\n# #     multiplied_coords = coord * 3\n# #     polygons.append(np.array(multiplied_coords, np.int32))\n# # coordinates = [\n# #     np.array([\n# # [676*3, 57*3],[916*3, 122*3],[811*3, 302*3],[631*3, 122*3],[676*3, 57*3]\n# # ])\n# # ]\n\n# polygons = []\n\n# for coord in coordinates:\n#     polygons.append(np.array(coord, np.int32))\n\npolygons = [\n    np.array([\n        [0, 0],\n        [1280 - 5, 0],\n        [1280 - 5, 1200 - 5],\n        [0, 1200 - 5]\n    ], np.int32)\n#     np.array([\n#         [1080 + 5, 0],\n#         [2160, 0],\n#         [2160, 1300 - 5],\n#         [1080 + 5, 1300 - 5]\n#     ], np.int32), \n#     np.array([\n#         [0, 1300 + 5],\n#         [1080 - 5, 1300 + 5],\n#         [1080 - 5, 3840],\n#         [0, 3840]\n#     ], np.int32), \n#     np.array([\n#         [1080 + 5, 1300 + 5],\n#         [2160, 1300 + 5],\n#         [2160, 3840],\n#         [1080 + 5, 3840]\n#     ], np.int32)\n]\n\nvideo_info = sv.VideoInfo.from_video_path(data3)\n\nzones = [\n    sv.PolygonZone(\n        polygon=polygon,\n        frame_resolution_wh=video_info.resolution_wh\n    )\n    for polygon\n    in polygons\n]\nzone_annotators = [\n    sv.PolygonZoneAnnotator(\n        zone=zone,\n        color=colors.by_idx(index),\n        thickness=6,\n        text_thickness=8,\n        text_scale=4\n    )\n    for index, zone\n    in enumerate(zones)\n]\nbox_annotators = [\n    sv.BoxAnnotator(\n        color=colors.by_idx(index),\n        thickness=4,\n        text_thickness=4,\n        text_scale=2\n        )\n    for index\n    in range(len(polygons))\n]\n\n# extract video frame\ngenerator = sv.get_video_frames_generator(data3)\niterator = iter(generator)\nframe = next(iterator)\n\n# detect\nresults = model(frame, size=1280)\ndetections = sv.Detections.from_yolov5(results)\ndetections = detections[(detections.class_id == 19) & (detections.confidence > 0.1)]\n\nfor zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n    mask = zone.trigger(detections=detections)\n    detections_filtered = detections[mask]\n    frame = box_annotator.annotate(scene=frame, detections=detections_filtered, skip_label=False)\n    frame = zone_annotator.annotate(scene=frame)\n\n%matplotlib inline\nsv.show_frame_in_notebook(frame, (16, 16))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import cv2\n# import supervision as sv\n\n# np.bool = np.bool_\n\n# colors = sv.ColorPalette.default()\n\n# # Create a single zone covering the entire frame\n# frame_coordinates = np.array([\n#     [0, 0],\n#     [0, 1280],  # Assuming frame width is 1280\n#     [720, 1280],  # Assuming frame height is 720\n#     [720, 0],\n#     [0, 0]\n# ])\n\n# frame_zone = sv.PolygonZone(\n#     polygon=frame_coordinates,\n#     frame_resolution_wh=(720, 1280)  # Assuming frame resolution is 720x1280\n# )\n\n# # Define model (assuming it's already defined)\n# # model = YourModel()  # Replace YourModel() with the actual model\n\n# # Define data (assuming it's already defined)\n# # data = 'your_video_file.mp4'  # Replace 'your_video_file.mp4' with the actual video file path\n\n# # Define box annotator\n# box_annotator = sv.BoxAnnotator(\n#     color=colors.by_idx(0),  # Assuming only one zone, so color index is 0\n#     thickness=4,\n#     text_thickness=4,\n#     text_scale=2\n# )\n\n# # Extract video frame\n# generator = sv.get_video_frames_generator(data)\n# iterator = iter(generator)\n# frame = next(iterator)\n\n# # Resize frame to default size\n# frame = cv2.resize(frame, (1280, 720))  # Assuming default frame size is 1280x720\n\n# # Detect\n# results = model(frame, size=1280)\n# detections = sv.Detections.from_yolov5(results)\n# detections = detections[(detections.class_id == 14) & (detections.confidence > 0.1)]\n\n# # Trigger the zone (entire frame) to get detections within the frame\n# mask = frame_zone.trigger(detections=detections)\n# detections_filtered = detections[mask]\n\n# # Count all classes\n# class_count = len(detections_filtered)\n\n# # Annotate the detections on the frame\n# frame = box_annotator.annotate(scene=frame, detections=detections_filtered, skip_label=False)\n\n# # Display class count at top left\n# cv2.putText(frame, f'Class Count: {class_count}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n\n# # Display the frame with annotations\n# %matplotlib inline\n# sv.show_frame_in_notebook(frame, (16, 16))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# colors = sv.ColorPalette.default()\n# polygons = [\n#     np.array([\n#         [540,  985 ],\n#         [1620, 985 ],\n#         [2160, 1920],\n#         [1620, 2855],\n#         [540,  2855],\n#         [0,    1920]\n#     ], np.int32),\n#     np.array([\n#         [0,    1920],\n#         [540,  985 ],\n#         [0,    0   ]\n#     ], np.int32),\n#     np.array([\n#         [1620, 985 ],\n#         [2160, 1920],\n#         [2160,    0]\n#     ], np.int32),\n#     np.array([\n#         [540,  985 ],\n#         [0,    0   ],\n#         [2160, 0   ],\n#         [1620, 985 ]\n#     ], np.int32),\n#     np.array([\n#         [0,    1920],\n#         [0,    3840],\n#         [540,  2855]\n#     ], np.int32),\n#     np.array([\n#         [2160, 1920],\n#         [1620, 2855],\n#         [2160, 3840]\n#     ], np.int32),\n#     np.array([\n#         [1620, 2855],\n#         [540,  2855],\n#         [0,    3840],\n#         [2160, 3840]\n#     ], np.int32)\n# ]\n# video_info = sv.VideoInfo.from_video_path(MARKET_SQUARE_VIDEO_PATH)\n\n# zones = [\n#     sv.PolygonZone(\n#         polygon=polygon, \n#         frame_resolution_wh=video_info.resolution_wh\n#     )\n#     for polygon\n#     in polygons\n# ]\n# zone_annotators = [\n#     sv.PolygonZoneAnnotator(\n#         zone=zone, \n#         color=colors.by_idx(index), \n#         thickness=6,\n#         text_thickness=8,\n#         text_scale=4\n#     )\n#     for index, zone\n#     in enumerate(zones)\n# ]\n# box_annotators = [\n#     sv.BoxAnnotator(\n#         color=colors.by_idx(index), \n#         thickness=4, \n#         text_thickness=4, \n#         text_scale=2\n#         )\n#     for index\n#     in range(len(polygons))\n# ]\n\n# def process_frame(frame: np.ndarray, i) -> np.ndarray:\n#     print(i)\n#     # detect\n#     results = model(frame, size=1280)\n#     detections = sv.Detections.from_yolov5(results)\n#     detections = detections[(detections.class_id == 0) & (detections.confidence > 0.5)]\n\n#     for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n#         mask = zone.trigger(detections=detections)\n#         detections_filtered = detections[mask]\n#         frame = box_annotator.annotate(scene=frame, detections=detections_filtered, skip_label=True)\n#         frame = zone_annotator.annotate(scene=frame)\n\n#     return frame\n\n# sv.process_video(source_path=MARKET_SQUARE_VIDEO_PATH, target_path=f\"{HOME}/market-square-result.mp4\", callback=process_frame)\n\n# from IPython import display\n# display.clear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CATTLE SEGMENTATION /  DETECTION AND LIVE COUNTING \n**","metadata":{}},{"cell_type":"code","source":"!wget https://cdn.pixabay.com/video/2021/08/15/85146-587630751_small.mp4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics\n\nimport ultralytics\nultralytics.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-seg.pt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Configure the tracking parameters and run the tracker\nmodel = YOLO('yolov8x-seg.pt')\n\nresults = model.track(source=\"/kaggle/working/85146-587630751_small.mp4\",conf=0.3, iou=0.5, save=True, tracker=\"bytetrack.yaml\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom collections import defaultdict\nimport supervision as sv\nfrom ultralytics import YOLO\n\n# Load the YOLOv8 model\nmodel = YOLO('/kaggle/working/yolov8x-seg.pt')\n\n# Set up video capture\ncap = cv2.VideoCapture(\"/kaggle/working/85146-587630751_small.mp4\")\n\n\n# [\n# np.array([\n# [1214, -2],[1190, 1066]\n# ])\n# ]\n\n# Define the line coordinates\nSTART = sv.Point(1214, -2)\nEND = sv.Point(1190, 1066)\n\nvideolink = r\"/kaggle/working/85146-587630751_small.mp4\"\n\n# Store the track history\ntrack_history = defaultdict(lambda: [])\n\n# Create a dictionary to keep track of objects that have crossed the line\ncrossed_objects = {}\n\n# Open a video sink for the output video\nvideo_info = sv.VideoInfo.from_video_path(videolink)\nwith sv.VideoSink(videolink, video_info) as sink:\n    \n    while cap.isOpened():\n        success, frame = cap.read()\n\n        if success:\n            # Run YOLOv8 tracking on the frame, persisting tracks between frames\n            results = model.track(frame,conf = 0.3, classes=[18], persist=True, save=True, tracker=\"bytetrack.yaml\")\n\n            # Get the boxes and track IDs\n            boxes = results[0].boxes.xywh.cpu()\n            track_ids = results[0].boxes.id.int().cpu().tolist()\n\n            # Visualize the results on the frame\n            annotated_frame = results[0].plot()\n            # detections = sv.Detections.from_yolov8(results[0])\n            detections = sv.Detections.from_ultralytics(results[0])\n\n            # Plot the tracks and count objects crossing the line\n            for box, track_id in zip(boxes, track_ids):\n                x, y, w, h = box\n                track = track_history[track_id]\n                track.append((float(x), float(y)))  # x, y center point\n                if len(track) > 30:  # retain 30 tracks for 30 frames\n                    track.pop(0)\n\n                # Check if the object crosses the line\n                if START.x < x < END.x and abs(y - START.y) < 5:  # Assuming objects cross horizontally\n                    if track_id not in crossed_objects:\n                        crossed_objects[track_id] = True\n\n                    # Annotate the object as it crosses the line\n                    cv2.rectangle(annotated_frame, (int(x - w / 2), int(y - h / 2)), (int(x + w / 2), int(y + h / 2)), (0, 255, 0), 2)\n\n            # Draw the line on the frame\n            cv2.line(annotated_frame, (START.x, START.y), (END.x, END.y), (0, 255, 0), 2)\n\n            # Write the count of objects on each frame\n            \n            count_text = f\"Objects crossed: {len(crossed_objects)}\"\n            cv2.putText(annotated_frame, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n            # Write the frame with annotations to the output video\n            sink.write_frame(annotated_frame)\n        else:\n            break\n\n# Release the video capture\ncap.release()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}